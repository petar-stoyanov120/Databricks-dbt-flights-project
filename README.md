# âœˆï¸ End-to-End Flight Data Engineering Pipeline  
**Databricks | Medallion Architecture | Incremental Processing**

---

## ğŸ“Œ Project Overview
This project demonstrates an **end-to-end data engineering pipeline** built on **Databricks**, following the **Medallion Architecture** pattern.

The pipeline ingests raw flight booking data, processes it incrementally through **Bronze, Silver, and Gold layers**, and produces **analytics-ready fact and dimension tables**.

> âš ï¸ **Note:**  
> `dbt` is planned for the analytics layer but **has not yet been integrated** into the repository.

---

## ğŸ—ï¸ High-Level Architecture
The data flows through the following stages:

1. **Raw Zone**
   - CSV files landed in **Databricks Volumes**

2. **Bronze Layer**
   - Incremental ingestion using **Databricks Autoloader**
   - Implemented via PySpark structured streaming

3. **Silver Layer**
   - Data cleansing and validation using  
     **Lakeflow Declarative Pipelines (DLT)**
   - Enforces data quality expectations

4. **Gold Layer**
   - Star Schema modeling (Facts & Dimensions)
   - Implements **Slowly Changing Dimensions (SCD Type 1)**

5. **Analytics Layer (Planned)**
   - Business transformations using **dbt Cloud**

---

## ğŸ§° Technical Stack

### Platform
- **Databricks (Free Edition)**

### Storage & Governance
- **Delta Lake**
- **Unity Catalog**
- **Databricks Volumes**

### Processing
- **PySpark**
- **Spark Structured Streaming**

### Orchestration
- **Databricks Jobs**
- **Lakeflow (DLT)**

### Analytics (Planned)
- **dbt Cloud**
- **Databricks SQL Warehouse**

---

## ğŸ§  Key Engineering Concepts Demonstrated

### ğŸ”¹ Medallion Architecture
- **Bronze:** Raw, incremental ingestion  
- **Silver:** Cleaned and validated datasets  
- **Gold:** Business-ready fact & dimension tables  

### ğŸ”¹ Incremental Processing
- Autoloader-based ingestion
- Efficient handling of new and changed data

### ğŸ”¹ Dynamic Pipeline Design
- Parameter-driven PySpark pipelines
- Reusable builders for dimensions and facts
- Avoids static, one-off notebooks

### ğŸ”¹ Data Quality Enforcement
- DLT expectations to:
  - Drop malformed records
  - Enforce schema and null checks

### ğŸ”¹ SCD Type 1
- Overwrites dimension records on change
- Maintains a single, current version of truth

---

## ğŸ“‚ Repository Structure

```text
Databricks&DBT End-To-End project/
â”‚
â”œâ”€â”€ SILVER_DLT_PIPELINE/
â”‚   â””â”€â”€ (DLT pipeline notebooks & logic)
â”‚
â”œâ”€â”€ BronzeLayer.py
â”œâ”€â”€ GOLD_FACT.py
â”œâ”€â”€ Gold_Dims.py
â”œâ”€â”€ Setup.py
â”œâ”€â”€ SrcParameters.py
â”‚
â”œâ”€â”€ dim_airports.csv
â”œâ”€â”€ dim_airports_increment.csv
â”œâ”€â”€ dim_airports_scd.csv
â”‚
â”œâ”€â”€ dim_flights.csv
â”œâ”€â”€ dim_flights_increment.csv
â”œâ”€â”€ dim_flights_scd.csv
â”‚
â”œâ”€â”€ dim_passengers.csv
â”œâ”€â”€ dim_passengers_increment.csv
â”œâ”€â”€ dim_passengers_scd.csv
â”‚
â”œâ”€â”€ fact_bookings.csv
â”‚
â””â”€â”€ README.md
